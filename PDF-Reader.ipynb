{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeph87Nz4rSyrAsJ3mpOvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omuskaikar/Langchain-PDF-reader/blob/main/PDF-Reader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google drive upload"
      ],
      "metadata": {
        "id": "rrwYRYz5jvlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6z6uFeFFem9",
        "outputId": "f23b630d-fb95-45f8-c98a-89eacb12c967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.273-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.26-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.2.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.273 langsmith-0.0.26 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m738.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.9\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.10.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Installing collected packages: filetype, python-magic, emoji, unstructured\n",
            "Successfully installed emoji-2.8.0 filetype-1.2.0 python-magic-0.4.27 unstructured-0.10.5\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone ,Weaviate, FAISS"
      ],
      "metadata": {
        "id": "5Ok6gDrdFvgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI API key as an environment variable for authentication\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"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\""
      ],
      "metadata": {
        "id": "ftxNPATdGd8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI API key as an environment variable for authentication\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "root_dir = '/content/gdrive/My Drive/data/'"
      ],
      "metadata": {
        "id": "Yp04d96FLkQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9937081f-25cd-4245-f8d2-b80b1fb5f4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the specified PDF folder\n",
        "pdf_folder_path = f'{root_dir}'\n",
        "os.listdir(pdf_folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPU3xcgPYOYj",
        "outputId": "65d8caeb-755c-40d4-8f77-0792dbbae834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Om_Manish Uskaikar_Resume_27-06-2023-11-26-20.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the specified PDF folder\n",
        "from langchain.document_loaders import UnstructuredPDFLoader"
      ],
      "metadata": {
        "id": "smpFlIGDZoTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF files as unstructured text data using 'UnstructuredPDFLoader'\n",
        "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path,fn)) for fn in os.listdir(pdf_folder_path)]"
      ],
      "metadata": {
        "id": "qXigAIFaYe1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB29OMjqZxC4",
        "outputId": "62ff8b2b-e8cb-44b1-d6a5-c8214cd163f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<langchain.document_loaders.pdf.UnstructuredPDFLoader at 0x7f1feb0ee950>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install 'tiktoken' package for tokenization tasks\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrJSBi9JeEtL",
        "outputId": "1f834cad-8120-4bd1-a76e-ba33e901309a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "Avgg5BqCcNs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRzrUNAV-1mg",
        "outputId": "e7bc5a2a-6318-4fb1-f7e1-650ce70da83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.3-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.0/399.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Collecting chroma-hnswlib==0.7.1 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2273169 sha256=6a486181d6cbfb4f90f755a8a8cc88e5f53a73e7df0ff0b80d232989f80243ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=a4848ca08b93752bacfe163a842ed51e91cb41dd9af676f82575dd72f1db3f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: tokenizers, pypika, monotonic, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.3 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 pypika-0.48.9 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tokenizers-0.13.3 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hETQmbn6aLB",
        "outputId": "8447b7b2-d499-409d-aa6b-588c0dd23a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index of vector representations from the loaded text data\n",
        "index = VectorstoreIndexCreator().from_loaders(loaders)\n",
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj5R_fpzZzp5",
        "outputId": "41ce9010-ba10-44c4-a3c7-54659d594a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreIndexWrapper(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f1fe17c89a0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search on the 'index' object with the given query\n",
        "index.query(\"is the candidate good for data analyst?\")"
      ],
      "metadata": {
        "id": "fheY3JkWaPZ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cde8a40b-ecba-493c-b7df-36f246b01990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYes, the candidate is well-suited for a data analyst role. They have a Btech in Computer Science and Engineering (Data Science) from Dwarkadas J. Sanghvi College of Engineering, and have experience with programming languages such as Python, C, HTML, CSS, JavaScript, MERN stack, Tableau, and My SQL. They have also completed courses in Python, Intro to Machine Learning, and Data Visualizations, and have an AWS Academy Graduate certification in AWS Academy Cloud Foundations. They have experience with data preprocessing and modelling, automating the workflow, and improving code readability and maintainability. They have also worked on projects such as Global Analysis of Fatal Diseases and Profit Prediction for Super Store.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Local upload"
      ],
      "metadata": {
        "id": "dZeRomDdL7sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DnIUTIgL6jg",
        "outputId": "7d8d651c-d58e-4bd7-d268-f3150e066851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.247)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.15)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"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\""
      ],
      "metadata": {
        "id": "ySkpyZmwMNjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import openai\n",
        "import requests"
      ],
      "metadata": {
        "id": "12txXO7JMdun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "pdfllm = OpenAI(max_tokens=100)"
      ],
      "metadata": {
        "id": "Ol8nALYKMuic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone ,Weaviate, FAISS"
      ],
      "metadata": {
        "id": "mj9xz0jHM6FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "rpJt0H4NNAmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PdfReader('/Om_Manish Uskaikar_Resume_27-06-2023-11-26-20.pdf')"
      ],
      "metadata": {
        "id": "xqOZZxO3NQck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = ''\n",
        "for i,page in enumerate(reader.pages):\n",
        "  text = page.extract_text()\n",
        "  if text:\n",
        "    raw_text += text"
      ],
      "metadata": {
        "id": "NKe6a_5mNYgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "NSswiTBI9kFS",
        "outputId": "7eb4a033-b3a6-4950-c486-dec74f55ee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Om Manish Uskaikar\\nData Scientist\\nomuskaikar@gmail.com +919326334540 omuskaikar\\nEDUCATION\\nDwarkadas J. Sanghvi College of Engineering\\nBtech in Computer Science and Engineering(Data Science)\\n•SGPA : 9.38 (sem 1)2021 – 2025\\nmumbai, India\\n•SGPA : 9.43 (sem 2)\\n•SGPA : 9.71  (sem 3)\\nNirmala Memorial Foundation Jr .College Of Commerce Science\\n•HSC : 95.12%2019 – 2021\\nmumbai, India\\nS.T. Lawrence High School\\n•SSC : 90.40%2009 – 2019\\nmumbai, India\\nPROFESSIONAL EXPERIENCE\\nJ.P Morgan External Engagement Program\\nA program started by J.P Morgan for top 60 students in engineering colleges to \\nlearn and develop professional skills.present\\nCERTIFICATES\\nPython Course (Kaggle Learn)                                                                                                                                 (2022)\\nIntro to Machine Learning(Kaggle Learn)                                                                                                            (2022)\\nData Visualizations (Kaggle Learn)                                                                                                                        (2022)\\nAWS Academy Graduate - AWS Academy Cloud Foundations                                                                        (2023)\\nPROFILE\\nPROGRAMMING SKILLS\\nPython programming\\nMERN stack\\nMachine Learning\\n(Sklearn , Numpy , Pandas , \\nMatplotlib , Pickle , Seaborn)C programming\\nTableauHTML , CSS , JavaScript\\nMy SQL\\nPROJECTS\\nGlobal Analysis of Fatal Diseases\\n• A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained \\ncan generate useful insights into spread and prevalence of diseases in different regions and analyzing their \\nhistorical trends.\\n1 / 2•Currently working to publish a research paper regarding the same.\\n•Performed CRUD operations with SQL on the processed dataset.\\n•Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\\nProfit Prediction for Super Store\\n•The project required predicting shop profitability and estimation of profit values. This was achieved by \\napplying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost, \\nLightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor, \\nRandom Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\\n•Robust data preprocessing techniques were employed, such as label encoding for categorical features and \\noutlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\\n•Accurate evaluation of models were achieved by calculating metrics like accuracy for classification and root \\nmean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\\nperforming model.\\n•Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model \\nparameters, enhancing performance and predictive accuracy.\\n•The code showcases the use of pipelines to streamline data preprocessing and modelling, automating the \\nworkflow and improving code readability and maintainability.\\n•Utilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools \\nfor data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity \\nof the code implementation.\\nMovies Recommendation System (TMDB dataset)\\n•Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates, \\nensuring reliable data for NLP tasks.\\n•Selective Feature Extraction: Careful selection of relevant columns for movie tag creation, including \\n'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'.\\n•Advanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text \\ndata preparation.\\n•Semantic Tag Generation: Comprehensive movie tags created by intelligently combining 'overview', \\n'genres', 'keywords', 'cast', and 'crew' data for precise similarity calculations.\\n•Recommendation System: Utilizing cosine similarity for accurate identification of similar movies based on \\nvectorized tag representations, enabling personalized recommendations.\\n•Deployed using streamlit.\\n2 / 2\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "            separator = \"\\n\",\n",
        "            chunk_size = 1000,\n",
        "            chunk_overlap = 100,\n",
        "            length_function = len,)\n",
        "texts= text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "Q7O3gZ5ANqDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "gDf8ASttONHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_texts(texts,embeddings)"
      ],
      "metadata": {
        "id": "oWxd8UZXOTiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(),chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "KSUYWhJ5OjMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is his best performance in engg?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs,question = query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QvC701MtO57c",
        "outputId": "51da7816-1633-49c3-a393-9af4085db6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Om Uskaikar's best performance in engineering is a SGPA of 9.71.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDFloader"
      ],
      "metadata": {
        "id": "my1QgPXxHe7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T12GmvhRLHun",
        "outputId": "67566cb4-a939-4371-e03b-2df680494611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import rarfile\n",
        "\n",
        "def extract_pdf_files(directory):\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.pdf'):\n",
        "                pdf_files.append({'Name': file, 'Path': os.path.join(root, file)})\n",
        "            elif file.endswith('.zip'):\n",
        "                zip_path = os.path.join(root, file)\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    for zip_file in zip_ref.namelist():\n",
        "                        if zip_file.endswith('.pdf'):\n",
        "                            pdf_files.append({'Name': zip_file, 'Path': os.path.join(root, zip_file)})\n",
        "            elif file.endswith('.rar'):\n",
        "                rar_path = os.path.join(root, file)\n",
        "                with rarfile.RarFile(rar_path, 'r') as rar_ref:\n",
        "                    for rar_file in rar_ref.namelist():\n",
        "                        if rar_file.endswith('.pdf'):\n",
        "                            pdf_files.append({'Name': rar_file, 'Path': os.path.join(root, rar_file)})\n",
        "    return pdf_files\n",
        "\n",
        "# Example usage\n",
        "directory_path = input('Enter the directory path: ')\n",
        "pdf_files = extract_pdf_files(directory_path)\n",
        "\n",
        "df = pd.DataFrame(pdf_files)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "hP4ZPrqzLDwH",
        "outputId": "07865016-29cb-4fb2-a545-7cd9e99ef80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the directory path: /content/sample_data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-508c20d3-90a8-4726-a884-c7e226c9e57e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-508c20d3-90a8-4726-a884-c7e226c9e57e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-5274dad6-afbd-410d-8e8e-c4c6639a4527\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5274dad6-afbd-410d-8e8e-c4c6639a4527')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-5274dad6-afbd-410d-8e8e-c4c6639a4527 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-508c20d3-90a8-4726-a884-c7e226c9e57e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-508c20d3-90a8-4726-a884-c7e226c9e57e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ab8433c-4ec2-4116-8f70-beba7543b03f",
        "id": "KrqGjxjbH1YQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.273)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.26)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.2.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Collecting llama_index\n",
            "  Downloading llama_index-0.8.9-py3-none-any.whl (702 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.4/702.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.4.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.5.14)\n",
            "Requirement already satisfied: langchain>=0.0.262 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.0.273)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.27.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Collecting urllib3<2 (from llama_index)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.11.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (4.0.3)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (0.0.26)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (2.8.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (2.2.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index) (2.31.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama_index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama_index) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama_index) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index) (1.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index) (2.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index) (2023.7.22)\n",
            "Installing collected packages: urllib3, llama_index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed llama_index-0.8.9 urllib3-1.26.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"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\""
      ],
      "metadata": {
        "id": "yirV28NlH1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import openai\n",
        "import requests"
      ],
      "metadata": {
        "id": "-4BF--dXH1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "pdfllm = OpenAI(max_tokens=100)"
      ],
      "metadata": {
        "id": "PjaI4oc3H1YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone ,Weaviate, FAISS"
      ],
      "metadata": {
        "id": "jGnk0qHKH1YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "fSFp6ZFkH1YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preforms OCR\n",
        "from pathlib import Path\n",
        "from llama_index import download_loader\n",
        "!pip install pytesseract\n",
        "\n",
        "\n",
        "ImageReader = download_loader(\"ImageReader\")\n",
        "imageLoader = ImageReader(text_type=\"plain_text\")\n",
        "FlatPdfReader = download_loader(\"FlatPdfReader\")\n",
        "pdfLoader = FlatPdfReader(image_loader=imageLoader)\n",
        "\n",
        "document = pdfLoader.load_data(file=Path('/content/Om_Manish Uskaikar_Resume_27-06-2023-11-26-20.pdf'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qNIGrXMPfWN",
        "outputId": "76d04f9d-2ac4-423a-d547-a6e4e955383f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VeGkfmb7efK",
        "outputId": "22ecdad5-dc78-4dca-88bf-733f334d5003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='60ddce03-9315-48b5-b16d-345b4ba30de8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cc4647c3a8d3f0be34df1b23f5bce13f8670b4ded10278d77d9a4671c2017149', text=\"Om Manish Uskaikar\\n\\nData Scientist\\n\\nomuskaikar@gmail.com , +919326334540 ) omuskaikar\\n\\n \\n\\nTj EDUCATION\\n\\nDwarkadas J. Sanghvi College of Engineering 2021 — 2025\\nBtech in Computer Science and Engineering(Data Science) mumbai, India\\n* SGPA: 9.38 (sem 1)\\n* SGPA: 9.43 (sem 2)\\n* SGPA: 9.71 (sem 3)\\n\\nNirmala Memorial Foundation Jr .College Of Commerce Science 2019 — 2021\\n« HSC : 95.12% mumbai, India\\nS.T. Lawrence High School 2009 — 2019\\n« SSC: 90.40% mumbai, India\\n\\n& PROFESSIONAL EXPERIENCE\\nJ.P Morgan External Engagement Program (7 present\\n\\nA program started by J.P Morgan for top 60 students in engineering colleges to\\nlearn and develop professional skills.\\n\\nQj CERTIFICATES\\n\\n* Python Course (Kaggle Learn) (2022) @\\n\\nIntro to Machine Learning(Kaggle Learn) (2022) 3\\n\\n* Data Visualizations (Kaggle Learn) (2022) 2\\n\\n« AWS Academy Graduate - AWS Academy Cloud Foundations (2023) @\\n6 PROFILE\\n\\n@ PROGRAMMING SKILLS\\n\\nPython programming C programming HTML, CSS, JavaScript\\nMERN stack Tableau MySQL\\nMachine Learning\\n\\n(Sklearn , Numpy , Pandas ,\\nMatplotlib , Pickle , Seaborn)\\n\\nfm PROJECTS\\n\\nGlobal Analysis of Fatal Diseases 7\\n\\n* A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained\\ncan generate useful insights into spread and prevalence of diseases in different regions and analyzing their\\nhistorical trends.\\n\\n1/2\\n\\x0c* Currently working to publish a research paper regarding the same.\\n+ Performed CRUD operations with SQL on the processed dataset.\\n+ Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\\n\\nProfit Prediction for Super Store 7\\n\\nThe project required predicting shop profitability and estimation of profit values. This was achieved by\\napplying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost,\\nLightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor,\\nRandom Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\\n\\nRobust data preprocessing techniques were employed, such as label encoding for categorical features and\\noutlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\\nAccurate evaluation of models were achieved by calculating metrics like accuracy for classification and root\\nmean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\\nperforming model.\\n\\nHyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model\\nparameters, enhancing performance and predictive accuracy.\\n\\nThe code showcases the use of pipelines to streamline data preprocessing and modelling, automating the\\nworkflow and improving code readability and maintainability.\\n\\nUtilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools\\nfor data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity\\nof the code implementation.\\n\\nMovies Recommendation System (TMDB dataset) 7\\n\\n« Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates,\\nensuring reliable data for NLP tasks.\\n\\nSelective Feature Extraction: Careful selection of relevant columns for movie tag creation, including\\n‘movie_id! 'title! ‘overview’, ‘genres! ‘keywords’ ‘cast’, and 'crew'.\\n\\nAdvanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text\\ndata preparation.\\n\\nSemantic Tag Generation: Comprehensive movie tags created by intelligently combining ‘overview!\\n‘genres’, 'keywords; ‘cast’ and 'crew' data for precise similarity calculations.\\n\\nRecommendation System: Utilizing cosine similarity for accurate identification of similar movies based on\\nvectorized tag representations, enabling personalized recommendations.\\n\\nDeployed using streamlit.\\n\\n \\n\\n2/2\\n\\x0c\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = document.text\n",
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "n6P3wAgVRQyz",
        "outputId": "85a2cd67-b214-45b3-90a7-d6c312645799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Om Manish Uskaikar\\n\\nData Scientist\\n\\nomuskaikar@gmail.com , +919326334540 ) omuskaikar\\n\\n \\n\\nTj EDUCATION\\n\\nDwarkadas J. Sanghvi College of Engineering 2021 — 2025\\nBtech in Computer Science and Engineering(Data Science) mumbai, India\\n* SGPA: 9.38 (sem 1)\\n* SGPA: 9.43 (sem 2)\\n* SGPA: 9.71 (sem 3)\\n\\nNirmala Memorial Foundation Jr .College Of Commerce Science 2019 — 2021\\n« HSC : 95.12% mumbai, India\\nS.T. Lawrence High School 2009 — 2019\\n« SSC: 90.40% mumbai, India\\n\\n& PROFESSIONAL EXPERIENCE\\nJ.P Morgan External Engagement Program (7 present\\n\\nA program started by J.P Morgan for top 60 students in engineering colleges to\\nlearn and develop professional skills.\\n\\nQj CERTIFICATES\\n\\n* Python Course (Kaggle Learn) (2022) @\\n\\nIntro to Machine Learning(Kaggle Learn) (2022) 3\\n\\n* Data Visualizations (Kaggle Learn) (2022) 2\\n\\n« AWS Academy Graduate - AWS Academy Cloud Foundations (2023) @\\n6 PROFILE\\n\\n@ PROGRAMMING SKILLS\\n\\nPython programming C programming HTML, CSS, JavaScript\\nMERN stack Tableau MySQL\\nMachine Learning\\n\\n(Sklearn , Numpy , Pandas ,\\nMatplotlib , Pickle , Seaborn)\\n\\nfm PROJECTS\\n\\nGlobal Analysis of Fatal Diseases 7\\n\\n* A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained\\ncan generate useful insights into spread and prevalence of diseases in different regions and analyzing their\\nhistorical trends.\\n\\n1/2\\n\\x0c* Currently working to publish a research paper regarding the same.\\n+ Performed CRUD operations with SQL on the processed dataset.\\n+ Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\\n\\nProfit Prediction for Super Store 7\\n\\nThe project required predicting shop profitability and estimation of profit values. This was achieved by\\napplying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost,\\nLightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor,\\nRandom Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\\n\\nRobust data preprocessing techniques were employed, such as label encoding for categorical features and\\noutlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\\nAccurate evaluation of models were achieved by calculating metrics like accuracy for classification and root\\nmean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\\nperforming model.\\n\\nHyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model\\nparameters, enhancing performance and predictive accuracy.\\n\\nThe code showcases the use of pipelines to streamline data preprocessing and modelling, automating the\\nworkflow and improving code readability and maintainability.\\n\\nUtilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools\\nfor data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity\\nof the code implementation.\\n\\nMovies Recommendation System (TMDB dataset) 7\\n\\n« Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates,\\nensuring reliable data for NLP tasks.\\n\\nSelective Feature Extraction: Careful selection of relevant columns for movie tag creation, including\\n‘movie_id! 'title! ‘overview’, ‘genres! ‘keywords’ ‘cast’, and 'crew'.\\n\\nAdvanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text\\ndata preparation.\\n\\nSemantic Tag Generation: Comprehensive movie tags created by intelligently combining ‘overview!\\n‘genres’, 'keywords; ‘cast’ and 'crew' data for precise similarity calculations.\\n\\nRecommendation System: Utilizing cosine similarity for accurate identification of similar movies based on\\nvectorized tag representations, enabling personalized recommendations.\\n\\nDeployed using streamlit.\\n\\n \\n\\n2/2\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NO OCR\n",
        "from pathlib import Path\n",
        "from llama_index import download_loader\n",
        "\n",
        "# Install or upgrade 'cryptography' to resolve the version conflict\n",
        "!pip install cryptography\n",
        "\n",
        "# Download the PDFMinerReader loader\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "# Create the loader instance and load the PDF data\n",
        "loader = PDFReader()\n",
        "documents = loader.load_data(file=Path('/content/Om_Manish Uskaikar_Resume_27-06-2023-11-26-20.pdf'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx-Vt1mhIdqi",
        "outputId": "ccfc976e-862b-454c-e1ba-61ba21485267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = ''\n",
        "for doc in documents:\n",
        "    raw_text += doc.get_content()"
      ],
      "metadata": {
        "id": "H-Q77uaCWver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "77295e29-8574-4540-c9f4-dca51d765e65",
        "id": "XyFj1s6kWves"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Om Manish Uskaikar\\nData Scientist\\nomuskaikar@gmail.com +919326334540 omuskaikar\\nEDUCATION\\nDwarkadas J. Sanghvi College of Engineering\\nBtech in Computer Science and Engineering(Data Science)\\n•SGPA : 9.38 (sem 1)2021 – 2025\\nmumbai, India\\n•SGPA : 9.43 (sem 2)\\n•SGPA : 9.71  (sem 3)\\nNirmala Memorial Foundation Jr .College Of Commerce Science\\n•HSC : 95.12%2019 – 2021\\nmumbai, India\\nS.T. Lawrence High School\\n•SSC : 90.40%2009 – 2019\\nmumbai, India\\nPROFESSIONAL EXPERIENCE\\nJ.P Morgan External Engagement Program\\nA program started by J.P Morgan for top 60 students in engineering colleges to \\nlearn and develop professional skills.present\\nCERTIFICATES\\nPython Course (Kaggle Learn)                                                                                                                                 (2022)\\nIntro to Machine Learning(Kaggle Learn)                                                                                                            (2022)\\nData Visualizations (Kaggle Learn)                                                                                                                        (2022)\\nAWS Academy Graduate - AWS Academy Cloud Foundations                                                                        (2023)\\nPROFILE\\nPROGRAMMING SKILLS\\nPython programming\\nMERN stack\\nMachine Learning\\n(Sklearn , Numpy , Pandas , \\nMatplotlib , Pickle , Seaborn)C programming\\nTableauHTML , CSS , JavaScript\\nMy SQL\\nPROJECTS\\nGlobal Analysis of Fatal Diseases\\n• A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained \\ncan generate useful insights into spread and prevalence of diseases in different regions and analyzing their \\nhistorical trends.\\n1 / 2•Currently working to publish a research paper regarding the same.\\n•Performed CRUD operations with SQL on the processed dataset.\\n•Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\\nProfit Prediction for Super Store\\n•The project required predicting shop profitability and estimation of profit values. This was achieved by \\napplying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost, \\nLightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor, \\nRandom Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\\n•Robust data preprocessing techniques were employed, such as label encoding for categorical features and \\noutlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\\n•Accurate evaluation of models were achieved by calculating metrics like accuracy for classification and root \\nmean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\\nperforming model.\\n•Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model \\nparameters, enhancing performance and predictive accuracy.\\n•The code showcases the use of pipelines to streamline data preprocessing and modelling, automating the \\nworkflow and improving code readability and maintainability.\\n•Utilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools \\nfor data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity \\nof the code implementation.\\nMovies Recommendation System (TMDB dataset)\\n•Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates, \\nensuring reliable data for NLP tasks.\\n•Selective Feature Extraction: Careful selection of relevant columns for movie tag creation, including \\n'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'.\\n•Advanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text \\ndata preparation.\\n•Semantic Tag Generation: Comprehensive movie tags created by intelligently combining 'overview', \\n'genres', 'keywords', 'cast', and 'crew' data for precise similarity calculations.\\n•Recommendation System: Utilizing cosine similarity for accurate identification of similar movies based on \\nvectorized tag representations, enabling personalized recommendations.\\n•Deployed using streamlit.\\n2 / 2\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "            separator = \"\\n\",\n",
        "            chunk_size = 1000,\n",
        "            chunk_overlap = 100,\n",
        "            length_function = len,)\n",
        "texts= text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "KMZaejp4H1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "KJxrCWpWH1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performs similarity search\n",
        "docsearch = FAISS.from_texts(texts,embeddings)"
      ],
      "metadata": {
        "id": "h7y-dQTPH1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(),chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "VKjklLX5H1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is sem 3 sgpa?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs,question = query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fac3af3c-92ca-4301-97f8-f9dcf5ac684c",
        "id": "sa5CQeBIH1Yb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 9.71'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query = input(\"Enter your question:  \")\n",
        "    if query.lower().strip() == \"stop\":\n",
        "        break\n",
        "\n",
        "    # Perform similarity search to find relevant documents for the given query\n",
        "    docs = docsearch.similarity_search(query)\n",
        "\n",
        "    # Execute the question-answering language chain to find the answer to the query\n",
        "    print(\"Ans:  \"+chain.run(input_documents=docs, question=query),end=\"\\n\\n\")\n",
        "print(\"stopped\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eG6Rj2rL0_r",
        "outputId": "5858d103-136b-479a-ebab-57f16414a160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question:  how is om?\n",
            "Ans:   I don't know.\n",
            "\n",
            "Enter your question:  stop\n",
            "stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#summarizer"
      ],
      "metadata": {
        "id": "xV3xUw-fGD2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "id": "eQLcYUZYGDKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "h5pCCIdtGkq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "Q3tgowqxGI5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "FmQuSWZgGO5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw_text"
      ],
      "metadata": {
        "id": "Lfmq5X-UHw8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqTable = dict()\n",
        "for word in words:\n",
        "\tword = word.lower()\n",
        "\tif word in stopWords:\n",
        "\t\tcontinue\n",
        "\tif word in freqTable:\n",
        "\t\tfreqTable[word] += 1\n",
        "\telse:\n",
        "\t\tfreqTable[word] = 1\n",
        "\n",
        "# Creating a dictionary to keep the score\n",
        "# of each sentence\n",
        "sentences = sent_tokenize(text)\n",
        "sentenceValue = dict()\n",
        "\n",
        "for sentence in sentences:\n",
        "\tfor word, freq in freqTable.items():\n",
        "\t\tif word in sentence.lower():\n",
        "\t\t\tif sentence in sentenceValue:\n",
        "\t\t\t\tsentenceValue[sentence] += freq\n",
        "\t\t\telse:\n",
        "\t\t\t\tsentenceValue[sentence] = freq\n",
        "\n",
        "\n",
        "\n",
        "sumValues = 0\n",
        "for sentence in sentenceValue:\n",
        "\tsumValues += sentenceValue[sentence]\n",
        "\n",
        "# Average value of a sentence from the original text\n",
        "\n",
        "average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "# Storing sentences into our summary.\n",
        "summary = ''\n",
        "for sentence in sentences:\n",
        "\tif (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "\t\tsummary += \" \" + sentence\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "RoWT9l4pHVTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER(named entity recognition)"
      ],
      "metadata": {
        "id": "5uu_f31Xd_Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw_text\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "yHksOss3eLH0",
        "outputId": "222a04cd-4c63-4a92-89c3-849a7298b25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Om Manish Uskaikar\\nData Scientist\\nomuskaikar@gmail.com +919326334540 omuskaikar\\nEDUCATION\\nDwarkadas J. Sanghvi College of Engineering\\nBtech in Computer Science and Engineering(Data Science)\\n•SGPA : 9.38 (sem 1)2021 – 2025\\nmumbai, India\\n•SGPA : 9.43 (sem 2)\\n•SGPA : 9.71  (sem 3)\\nNirmala Memorial Foundation Jr .College Of Commerce Science\\n•HSC : 95.12%2019 – 2021\\nmumbai, India\\nS.T. Lawrence High School\\n•SSC : 90.40%2009 – 2019\\nmumbai, India\\nPROFESSIONAL EXPERIENCE\\nJ.P Morgan External Engagement Program\\nA program started by J.P Morgan for top 60 students in engineering colleges to \\nlearn and develop professional skills.present\\nCERTIFICATES\\nPython Course (Kaggle Learn)                                                                                                                                 (2022)\\nIntro to Machine Learning(Kaggle Learn)                                                                                                            (2022)\\nData Visualizations (Kaggle Learn)                                                                                                                        (2022)\\nAWS Academy Graduate - AWS Academy Cloud Foundations                                                                        (2023)\\nPROFILE\\nPROGRAMMING SKILLS\\nPython programming\\nMERN stack\\nMachine Learning\\n(Sklearn , Numpy , Pandas , \\nMatplotlib , Pickle , Seaborn)C programming\\nTableauHTML , CSS , JavaScript\\nMy SQL\\nPROJECTS\\nGlobal Analysis of Fatal Diseases\\n• A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained \\ncan generate useful insights into spread and prevalence of diseases in different regions and analyzing their \\nhistorical trends.\\n1 / 2•Currently working to publish a research paper regarding the same.\\n•Performed CRUD operations with SQL on the processed dataset.\\n•Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\\nProfit Prediction for Super Store\\n•The project required predicting shop profitability and estimation of profit values. This was achieved by \\napplying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost, \\nLightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor, \\nRandom Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\\n•Robust data preprocessing techniques were employed, such as label encoding for categorical features and \\noutlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\\n•Accurate evaluation of models were achieved by calculating metrics like accuracy for classification and root \\nmean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\\nperforming model.\\n•Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model \\nparameters, enhancing performance and predictive accuracy.\\n•The code showcases the use of pipelines to streamline data preprocessing and modelling, automating the \\nworkflow and improving code readability and maintainability.\\n•Utilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools \\nfor data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity \\nof the code implementation.\\nMovies Recommendation System (TMDB dataset)\\n•Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates, \\nensuring reliable data for NLP tasks.\\n•Selective Feature Extraction: Careful selection of relevant columns for movie tag creation, including \\n'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'.\\n•Advanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text \\ndata preparation.\\n•Semantic Tag Generation: Comprehensive movie tags created by intelligently combining 'overview', \\n'genres', 'keywords', 'cast', and 'crew' data for precise similarity calculations.\\n•Recommendation System: Utilizing cosine similarity for accurate identification of similar movies based on \\nvectorized tag representations, enabling personalized recommendations.\\n•Deployed using streamlit.\\n2 / 2\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "# Load the spaCy model (you'll need to download it first)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.pipeline\n",
        "# The text you want to analyze\n",
        "text = \"\"\"\n",
        "Om Manish Uskaikar\n",
        "Data Scientist\n",
        "omuskaikar@gmail.com +919326334540 omuskaikar\n",
        "EDUCATION\n",
        "Dwarkadas J. Sanghvi College of Engineering\n",
        "Btech in Computer Science and Engineering(Data Science)\n",
        "•SGPA : 9.38 (sem 1)2021 – 2025\n",
        "mumbai, India\n",
        "•SGPA : 9.43 (sem 2)\n",
        "•SGPA : 9.71  (sem 3)\n",
        "Nirmala Memorial Foundation Jr .College Of Commerce Science\n",
        "•HSC : 95.12%2019 – 2021\n",
        "mumbai, India\n",
        "S.T. Lawrence High School\n",
        "•SSC : 90.40%2009 – 2019\n",
        "mumbai, India\n",
        "PROFESSIONAL EXPERIENCE\n",
        "J.P Morgan External Engagement Program\n",
        "A program started by J.P Morgan for top 60 students in engineering colleges to\n",
        "learn and develop professional skills.present\n",
        "CERTIFICATES\n",
        "Python Course (Kaggle Learn)                                                                                                                                 (2022)\n",
        "Intro to Machine Learning(Kaggle Learn)                                                                                                            (2022)\n",
        "Data Visualizations (Kaggle Learn)                                                                                                                        (2022)\n",
        "AWS Academy Graduate - AWS Academy Cloud Foundations                                                                        (2023)\n",
        "PROFILE\n",
        "PROGRAMMING SKILLS\n",
        "Python programming\n",
        "MERN stack\n",
        "Machine Learning\n",
        "(Sklearn , Numpy , Pandas ,\n",
        "Matplotlib , Pickle , Seaborn)C programming\n",
        "TableauHTML , CSS , JavaScript\n",
        "My SQL\n",
        "PROJECTS\n",
        "Global Analysis of Fatal Diseases\n",
        "• A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained\n",
        "can generate useful insights into spread and prevalence of diseases in different regions and analyzing their\n",
        "historical trends.\n",
        "1 / 2•Currently working to publish a research paper regarding the same.\n",
        "•Performed CRUD operations with SQL on the processed dataset.\n",
        "•Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\n",
        "Profit Prediction for Super Store\n",
        "•The project required predicting shop profitability and estimation of profit values. This was achieved by\n",
        "applying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost,\n",
        "LightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor,\n",
        "Random Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\n",
        "•Robust data preprocessing techniques were employed, such as label encoding for categorical features and\n",
        "outlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\n",
        "•Accurate evaluation of models were achieved by calculating metrics like accuracy for classification and root\n",
        "mean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\n",
        "performing model.\n",
        "•Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model\n",
        "parameters, enhancing performance and predictive accuracy.\n",
        "•The code showcases the use of pipelines to streamline data preprocessing and modelling, automating the\n",
        "workflow and improving code readability and maintainability.\n",
        "•Utilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools\n",
        "for data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity\n",
        "of the code implementation.\n",
        "Movies Recommendation System (TMDB dataset)\n",
        "•Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates,\n",
        "ensuring reliable data for NLP tasks.\n",
        "•Selective Feature Extraction: Careful selection of relevant columns for movie tag creation, including\n",
        "'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'.\n",
        "•Advanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text\n",
        "data preparation.\n",
        "•Semantic Tag Generation: Comprehensive movie tags created by intelligently combining 'overview',\n",
        "'genres', 'keywords', 'cast', and 'crew' data for precise similarity calculations.\n",
        "•Recommendation System: Utilizing cosine similarity for accurate identification of similar movies based on\n",
        "vectorized tag representations, enabling personalized recommendations.\n",
        "•Deployed using streamlit.\n",
        "2 / 2\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with spaCy NLP pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "\n",
        "for token in doc:\n",
        "  print(token,\" | \",token.pos_,\" | \",token.lemma_)\n",
        "\n",
        "# Create a table to store the named entities and their types\n",
        "table_data = []\n",
        "for ent in doc.ents:\n",
        "    table_data.append((ent.text, ent.label_))\n",
        "\n",
        "# Convert the table_data to a DataFrame\n",
        "df = pd.DataFrame(table_data, columns=[\"Word\", \"Entity Type\"])\n",
        "\n",
        "# Print the DataFrame as a table\n",
        "df.head(300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daVYFlSJuTyO",
        "outputId": "fbcc2a7b-8296-4c1e-baf7-3cb34c3e94ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Om  |  PROPN  |  Om\n",
            "Manish  |  PROPN  |  Manish\n",
            "Uskaikar  |  PROPN  |  Uskaikar\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Data  |  PROPN  |  Data\n",
            "Scientist  |  PROPN  |  Scientist\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "omuskaikar@gmail.com  |  X  |  omuskaikar@gmail.com\n",
            "+919326334540  |  PROPN  |  +919326334540\n",
            "omuskaikar  |  PROPN  |  omuskaikar\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "EDUCATION  |  PROPN  |  EDUCATION\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Dwarkadas  |  PROPN  |  Dwarkadas\n",
            "J.  |  PROPN  |  J.\n",
            "Sanghvi  |  PROPN  |  Sanghvi\n",
            "College  |  PROPN  |  College\n",
            "of  |  ADP  |  of\n",
            "Engineering  |  PROPN  |  Engineering\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Btech  |  PROPN  |  Btech\n",
            "in  |  ADP  |  in\n",
            "Computer  |  PROPN  |  Computer\n",
            "Science  |  PROPN  |  Science\n",
            "and  |  CCONJ  |  and\n",
            "Engineering(Data  |  PROPN  |  Engineering(Data\n",
            "Science  |  PROPN  |  Science\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•SGPA  |  VERB  |  •SGPA\n",
            ":  |  PUNCT  |  :\n",
            "9.38  |  NUM  |  9.38\n",
            "(  |  PUNCT  |  (\n",
            "sem  |  NOUN  |  sem\n",
            "1)2021  |  NUM  |  1)2021\n",
            "–  |  PUNCT  |  –\n",
            "2025  |  NUM  |  2025\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "mumbai  |  NOUN  |  mumbai\n",
            ",  |  PUNCT  |  ,\n",
            "India  |  PROPN  |  India\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•SGPA  |  VERB  |  •SGPA\n",
            ":  |  PUNCT  |  :\n",
            "9.43  |  NUM  |  9.43\n",
            "(  |  PUNCT  |  (\n",
            "sem  |  NOUN  |  sem\n",
            "2  |  NUM  |  2\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•SGPA  |  VERB  |  •SGPA\n",
            ":  |  PUNCT  |  :\n",
            "9.71  |  NUM  |  9.71\n",
            "   |  SPACE  |   \n",
            "(  |  PUNCT  |  (\n",
            "sem  |  NOUN  |  sem\n",
            "3  |  NUM  |  3\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Nirmala  |  PROPN  |  Nirmala\n",
            "Memorial  |  PROPN  |  Memorial\n",
            "Foundation  |  PROPN  |  Foundation\n",
            "Jr  |  PROPN  |  Jr\n",
            ".College  |  NOUN  |  .college\n",
            "Of  |  ADP  |  of\n",
            "Commerce  |  PROPN  |  Commerce\n",
            "Science  |  PROPN  |  Science\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•HSC  |  NOUN  |  •hsc\n",
            ":  |  PUNCT  |  :\n",
            "95.12%2019  |  NUM  |  95.12%2019\n",
            "–  |  PUNCT  |  –\n",
            "2021  |  NUM  |  2021\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "mumbai  |  NOUN  |  mumbai\n",
            ",  |  PUNCT  |  ,\n",
            "India  |  PROPN  |  India\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "S.T.  |  PROPN  |  S.T.\n",
            "Lawrence  |  PROPN  |  Lawrence\n",
            "High  |  PROPN  |  High\n",
            "School  |  PROPN  |  School\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•SSC  |  NOUN  |  •ssc\n",
            ":  |  PUNCT  |  :\n",
            "90.40%2009  |  NUM  |  90.40%2009\n",
            "–  |  PUNCT  |  –\n",
            "2019  |  NUM  |  2019\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "mumbai  |  NOUN  |  mumbai\n",
            ",  |  PUNCT  |  ,\n",
            "India  |  PROPN  |  India\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "PROFESSIONAL  |  PROPN  |  PROFESSIONAL\n",
            "EXPERIENCE  |  PROPN  |  EXPERIENCE\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "J.P  |  PROPN  |  J.P\n",
            "Morgan  |  PROPN  |  Morgan\n",
            "External  |  PROPN  |  External\n",
            "Engagement  |  PROPN  |  Engagement\n",
            "Program  |  PROPN  |  Program\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "A  |  PRON  |  a\n",
            "program  |  NOUN  |  program\n",
            "started  |  VERB  |  start\n",
            "by  |  ADP  |  by\n",
            "J.P  |  PROPN  |  J.P\n",
            "Morgan  |  PROPN  |  Morgan\n",
            "for  |  ADP  |  for\n",
            "top  |  ADJ  |  top\n",
            "60  |  NUM  |  60\n",
            "students  |  NOUN  |  student\n",
            "in  |  ADP  |  in\n",
            "engineering  |  NOUN  |  engineering\n",
            "colleges  |  NOUN  |  college\n",
            "to  |  PART  |  to\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "learn  |  VERB  |  learn\n",
            "and  |  CCONJ  |  and\n",
            "develop  |  VERB  |  develop\n",
            "professional  |  ADJ  |  professional\n",
            "skills.present  |  NOUN  |  skills.present\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "CERTIFICATES  |  NOUN  |  certificate\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Python  |  PROPN  |  Python\n",
            "Course  |  NOUN  |  course\n",
            "(  |  PUNCT  |  (\n",
            "Kaggle  |  PROPN  |  Kaggle\n",
            "Learn  |  PROPN  |  Learn\n",
            ")  |  PUNCT  |  )\n",
            "                                                                                                                                  |  SPACE  |                                                                                                                                  \n",
            "(  |  PUNCT  |  (\n",
            "2022  |  NUM  |  2022\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Intro  |  PROPN  |  Intro\n",
            "to  |  ADP  |  to\n",
            "Machine  |  PROPN  |  Machine\n",
            "Learning(Kaggle  |  PROPN  |  Learning(Kaggle\n",
            "Learn  |  PROPN  |  Learn\n",
            ")  |  PUNCT  |  )\n",
            "                                                                                                             |  SPACE  |                                                                                                             \n",
            "(  |  PUNCT  |  (\n",
            "2022  |  NUM  |  2022\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Data  |  PROPN  |  Data\n",
            "Visualizations  |  PROPN  |  Visualizations\n",
            "(  |  PUNCT  |  (\n",
            "Kaggle  |  PROPN  |  Kaggle\n",
            "Learn  |  PROPN  |  Learn\n",
            ")  |  PUNCT  |  )\n",
            "                                                                                                                         |  SPACE  |                                                                                                                         \n",
            "(  |  PUNCT  |  (\n",
            "2022  |  NUM  |  2022\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "AWS  |  PROPN  |  AWS\n",
            "Academy  |  PROPN  |  Academy\n",
            "Graduate  |  PROPN  |  Graduate\n",
            "-  |  PUNCT  |  -\n",
            "AWS  |  PROPN  |  AWS\n",
            "Academy  |  PROPN  |  Academy\n",
            "Cloud  |  PROPN  |  Cloud\n",
            "Foundations  |  PROPN  |  Foundations\n",
            "                                                                         |  SPACE  |                                                                         \n",
            "(  |  PUNCT  |  (\n",
            "2023  |  NUM  |  2023\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "PROFILE  |  PROPN  |  PROFILE\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "PROGRAMMING  |  NOUN  |  programming\n",
            "SKILLS  |  PROPN  |  SKILLS\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Python  |  PROPN  |  Python\n",
            "programming  |  NOUN  |  programming\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "MERN  |  PROPN  |  MERN\n",
            "stack  |  VERB  |  stack\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Machine  |  PROPN  |  Machine\n",
            "Learning  |  PROPN  |  Learning\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "(  |  PUNCT  |  (\n",
            "Sklearn  |  PROPN  |  Sklearn\n",
            ",  |  PUNCT  |  ,\n",
            "Numpy  |  PROPN  |  Numpy\n",
            ",  |  PUNCT  |  ,\n",
            "Pandas  |  PROPN  |  Pandas\n",
            ",  |  PUNCT  |  ,\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Matplotlib  |  PROPN  |  Matplotlib\n",
            ",  |  PUNCT  |  ,\n",
            "Pickle  |  PROPN  |  Pickle\n",
            ",  |  PUNCT  |  ,\n",
            "Seaborn)C  |  PROPN  |  Seaborn)C\n",
            "programming  |  NOUN  |  programming\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "TableauHTML  |  NOUN  |  tableauhtml\n",
            ",  |  PUNCT  |  ,\n",
            "CSS  |  NOUN  |  css\n",
            ",  |  PUNCT  |  ,\n",
            "JavaScript  |  PROPN  |  JavaScript\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "My  |  PRON  |  my\n",
            "SQL  |  PROPN  |  SQL\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "PROJECTS  |  NOUN  |  project\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Global  |  PROPN  |  Global\n",
            "Analysis  |  PROPN  |  Analysis\n",
            "of  |  ADP  |  of\n",
            "Fatal  |  PROPN  |  Fatal\n",
            "Diseases  |  PROPN  |  Diseases\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•  |  NUM  |  •\n",
            "A  |  DET  |  a\n",
            "data  |  NOUN  |  datum\n",
            "visualization  |  NOUN  |  visualization\n",
            "project  |  NOUN  |  project\n",
            "on  |  ADP  |  on\n",
            "analyzing  |  VERB  |  analyze\n",
            "impact  |  NOUN  |  impact\n",
            "of  |  ADP  |  of\n",
            "fatal  |  ADJ  |  fatal\n",
            "diseases  |  NOUN  |  disease\n",
            "on  |  ADP  |  on\n",
            "various  |  ADJ  |  various\n",
            "countries  |  NOUN  |  country\n",
            ".  |  PUNCT  |  .\n",
            "Analysis  |  NOUN  |  analysis\n",
            "obtained  |  VERB  |  obtain\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "can  |  AUX  |  can\n",
            "generate  |  VERB  |  generate\n",
            "useful  |  ADJ  |  useful\n",
            "insights  |  NOUN  |  insight\n",
            "into  |  ADP  |  into\n",
            "spread  |  NOUN  |  spread\n",
            "and  |  CCONJ  |  and\n",
            "prevalence  |  NOUN  |  prevalence\n",
            "of  |  ADP  |  of\n",
            "diseases  |  NOUN  |  disease\n",
            "in  |  ADP  |  in\n",
            "different  |  ADJ  |  different\n",
            "regions  |  NOUN  |  region\n",
            "and  |  CCONJ  |  and\n",
            "analyzing  |  VERB  |  analyze\n",
            "their  |  PRON  |  their\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "historical  |  ADJ  |  historical\n",
            "trends  |  NOUN  |  trend\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "1  |  NUM  |  1\n",
            "/  |  SYM  |  /\n",
            "2•Currently  |  ADV  |  2•currently\n",
            "working  |  VERB  |  work\n",
            "to  |  PART  |  to\n",
            "publish  |  VERB  |  publish\n",
            "a  |  DET  |  a\n",
            "research  |  NOUN  |  research\n",
            "paper  |  NOUN  |  paper\n",
            "regarding  |  VERB  |  regard\n",
            "the  |  DET  |  the\n",
            "same  |  ADJ  |  same\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Performed  |  VERB  |  •performe\n",
            "CRUD  |  PROPN  |  CRUD\n",
            "operations  |  NOUN  |  operation\n",
            "with  |  ADP  |  with\n",
            "SQL  |  PROPN  |  SQL\n",
            "on  |  ADP  |  on\n",
            "the  |  DET  |  the\n",
            "processed  |  VERB  |  process\n",
            "dataset  |  NOUN  |  dataset\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Libraries  |  NOUN  |  •librarie\n",
            "used  |  VERB  |  use\n",
            ":  |  PUNCT  |  :\n",
            "Pandas  |  PROPN  |  Pandas\n",
            ",  |  PUNCT  |  ,\n",
            "numpy  |  PROPN  |  numpy\n",
            ",  |  PUNCT  |  ,\n",
            "matplotlib  |  PROPN  |  matplotlib\n",
            ",  |  PUNCT  |  ,\n",
            "seaborn  |  ADJ  |  seaborn\n",
            ",  |  PUNCT  |  ,\n",
            "barchart  |  ADJ  |  barchart\n",
            "race  |  NOUN  |  race\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Profit  |  PROPN  |  Profit\n",
            "Prediction  |  PROPN  |  Prediction\n",
            "for  |  ADP  |  for\n",
            "Super  |  PROPN  |  Super\n",
            "Store  |  PROPN  |  Store\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•The  |  PROPN  |  •The\n",
            "project  |  NOUN  |  project\n",
            "required  |  VERB  |  require\n",
            "predicting  |  VERB  |  predict\n",
            "shop  |  NOUN  |  shop\n",
            "profitability  |  NOUN  |  profitability\n",
            "and  |  CCONJ  |  and\n",
            "estimation  |  NOUN  |  estimation\n",
            "of  |  ADP  |  of\n",
            "profit  |  NOUN  |  profit\n",
            "values  |  NOUN  |  value\n",
            ".  |  PUNCT  |  .\n",
            "This  |  PRON  |  this\n",
            "was  |  AUX  |  be\n",
            "achieved  |  VERB  |  achieve\n",
            "by  |  ADP  |  by\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "applying  |  VERB  |  apply\n",
            "diverse  |  ADJ  |  diverse\n",
            "set  |  NOUN  |  set\n",
            "of  |  ADP  |  of\n",
            "classification  |  NOUN  |  classification\n",
            "models  |  NOUN  |  model\n",
            "(  |  PUNCT  |  (\n",
            "Logistic  |  PROPN  |  Logistic\n",
            "Regression  |  PROPN  |  Regression\n",
            ",  |  PUNCT  |  ,\n",
            "Decision  |  PROPN  |  Decision\n",
            "Tree  |  PROPN  |  Tree\n",
            ",  |  PUNCT  |  ,\n",
            "Random  |  PROPN  |  Random\n",
            "Forest  |  PROPN  |  Forest\n",
            ",  |  PUNCT  |  ,\n",
            "AdaBoost  |  PROPN  |  AdaBoost\n",
            ",  |  PUNCT  |  ,\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "LightGBM  |  NOUN  |  lightgbm\n",
            ",  |  PUNCT  |  ,\n",
            "XGBoost  |  PROPN  |  XGBoost\n",
            ",  |  PUNCT  |  ,\n",
            "SVM  |  PROPN  |  SVM\n",
            ",  |  PUNCT  |  ,\n",
            "KNN  |  PROPN  |  KNN\n",
            ")  |  PUNCT  |  )\n",
            "and  |  CCONJ  |  and\n",
            "regression  |  NOUN  |  regression\n",
            "models  |  NOUN  |  model\n",
            "(  |  PUNCT  |  (\n",
            "Linear  |  PROPN  |  Linear\n",
            "Regression  |  PROPN  |  Regression\n",
            ",  |  PUNCT  |  ,\n",
            "Decision  |  PROPN  |  Decision\n",
            "Tree  |  PROPN  |  Tree\n",
            "Regressor  |  PROPN  |  Regressor\n",
            ",  |  PUNCT  |  ,\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Random  |  PROPN  |  Random\n",
            "Forest  |  PROPN  |  Forest\n",
            "Regressor  |  PROPN  |  Regressor\n",
            ",  |  PUNCT  |  ,\n",
            "AdaBoost  |  PROPN  |  AdaBoost\n",
            "Regressor  |  PROPN  |  Regressor\n",
            ",  |  PUNCT  |  ,\n",
            "LightGBM  |  PROPN  |  LightGBM\n",
            "Regressor  |  PROPN  |  Regressor\n",
            ")  |  PUNCT  |  )\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Robust  |  ADP  |  •robust\n",
            "data  |  NOUN  |  datum\n",
            "preprocessing  |  VERB  |  preprocesse\n",
            "techniques  |  NOUN  |  technique\n",
            "were  |  AUX  |  be\n",
            "employed  |  VERB  |  employ\n",
            ",  |  PUNCT  |  ,\n",
            "such  |  ADJ  |  such\n",
            "as  |  ADP  |  as\n",
            "label  |  NOUN  |  label\n",
            "encoding  |  NOUN  |  encoding\n",
            "for  |  ADP  |  for\n",
            "categorical  |  ADJ  |  categorical\n",
            "features  |  NOUN  |  feature\n",
            "and  |  CCONJ  |  and\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "outlier  |  ADJ  |  outli\n",
            "removal  |  NOUN  |  removal\n",
            "based  |  VERB  |  base\n",
            "on  |  ADP  |  on\n",
            "Z  |  NOUN  |  z\n",
            "-  |  PUNCT  |  -\n",
            "scores  |  NOUN  |  score\n",
            ",  |  PUNCT  |  ,\n",
            "ensuring  |  VERB  |  ensure\n",
            "data  |  NOUN  |  data\n",
            "quality  |  NOUN  |  quality\n",
            "and  |  CCONJ  |  and\n",
            "improving  |  VERB  |  improve\n",
            "model  |  NOUN  |  model\n",
            "performance  |  NOUN  |  performance\n",
            "and  |  CCONJ  |  and\n",
            "reliability  |  NOUN  |  reliability\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Accurate  |  ADJ  |  •accurate\n",
            "evaluation  |  NOUN  |  evaluation\n",
            "of  |  ADP  |  of\n",
            "models  |  NOUN  |  model\n",
            "were  |  AUX  |  be\n",
            "achieved  |  VERB  |  achieve\n",
            "by  |  ADP  |  by\n",
            "calculating  |  VERB  |  calculate\n",
            "metrics  |  NOUN  |  metric\n",
            "like  |  ADP  |  like\n",
            "accuracy  |  NOUN  |  accuracy\n",
            "for  |  ADP  |  for\n",
            "classification  |  NOUN  |  classification\n",
            "and  |  CCONJ  |  and\n",
            "root  |  NOUN  |  root\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "mean  |  VERB  |  mean\n",
            "squared  |  ADJ  |  squared\n",
            "error  |  NOUN  |  error\n",
            "(  |  PUNCT  |  (\n",
            "RMSE  |  PROPN  |  RMSE\n",
            ")  |  PUNCT  |  )\n",
            "for  |  ADP  |  for\n",
            "regression  |  NOUN  |  regression\n",
            ",  |  PUNCT  |  ,\n",
            "enabling  |  VERB  |  enable\n",
            "effective  |  ADJ  |  effective\n",
            "comparison  |  NOUN  |  comparison\n",
            "and  |  CCONJ  |  and\n",
            "selection  |  NOUN  |  selection\n",
            "of  |  ADP  |  of\n",
            "the  |  DET  |  the\n",
            "best-  |  ADJ  |  best-\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "performing  |  VERB  |  perform\n",
            "model  |  NOUN  |  model\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Hyperparameter  |  NOUN  |  •hyperparameter\n",
            "tuning  |  NOUN  |  tuning\n",
            "was  |  AUX  |  be\n",
            "done  |  VERB  |  do\n",
            "using  |  VERB  |  use\n",
            "GridSearchCV  |  PROPN  |  GridSearchCV\n",
            "and  |  CCONJ  |  and\n",
            "RandomizedSearchCV  |  PROPN  |  RandomizedSearchCV\n",
            ",  |  PUNCT  |  ,\n",
            "optimized  |  VERB  |  optimize\n",
            "model  |  NOUN  |  model\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "parameters  |  NOUN  |  parameter\n",
            ",  |  PUNCT  |  ,\n",
            "enhancing  |  VERB  |  enhance\n",
            "performance  |  NOUN  |  performance\n",
            "and  |  CCONJ  |  and\n",
            "predictive  |  ADJ  |  predictive\n",
            "accuracy  |  NOUN  |  accuracy\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•The  |  PROPN  |  •The\n",
            "code  |  NOUN  |  code\n",
            "showcases  |  VERB  |  showcase\n",
            "the  |  DET  |  the\n",
            "use  |  NOUN  |  use\n",
            "of  |  ADP  |  of\n",
            "pipelines  |  NOUN  |  pipeline\n",
            "to  |  PART  |  to\n",
            "streamline  |  VERB  |  streamline\n",
            "data  |  NOUN  |  datum\n",
            "preprocessing  |  NOUN  |  preprocessing\n",
            "and  |  CCONJ  |  and\n",
            "modelling  |  NOUN  |  modelling\n",
            ",  |  PUNCT  |  ,\n",
            "automating  |  VERB  |  automate\n",
            "the  |  DET  |  the\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "workflow  |  NOUN  |  workflow\n",
            "and  |  CCONJ  |  and\n",
            "improving  |  VERB  |  improve\n",
            "code  |  NOUN  |  code\n",
            "readability  |  NOUN  |  readability\n",
            "and  |  CCONJ  |  and\n",
            "maintainability  |  NOUN  |  maintainability\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Utilization  |  NOUN  |  •utilization\n",
            "of  |  ADP  |  of\n",
            "powerful  |  ADJ  |  powerful\n",
            "libraries  |  NOUN  |  library\n",
            "(  |  PUNCT  |  (\n",
            "scikit  |  NOUN  |  scikit\n",
            "-  |  PUNCT  |  -\n",
            "learn  |  NOUN  |  learn\n",
            ",  |  PUNCT  |  ,\n",
            "XGBoost  |  PROPN  |  XGBoost\n",
            ",  |  PUNCT  |  ,\n",
            "LightGBM  |  NOUN  |  lightgbm\n",
            ",  |  PUNCT  |  ,\n",
            "pandas  |  VERB  |  panda\n",
            ",  |  PUNCT  |  ,\n",
            "plotly  |  ADV  |  plotly\n",
            ")  |  PUNCT  |  )\n",
            "provided  |  VERB  |  provide\n",
            "efficient  |  ADJ  |  efficient\n",
            "tools  |  NOUN  |  tool\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "for  |  ADP  |  for\n",
            "data  |  NOUN  |  datum\n",
            "manipulation  |  NOUN  |  manipulation\n",
            ",  |  PUNCT  |  ,\n",
            "modelling  |  NOUN  |  modelling\n",
            ",  |  PUNCT  |  ,\n",
            "visualization  |  NOUN  |  visualization\n",
            ",  |  PUNCT  |  ,\n",
            "and  |  CCONJ  |  and\n",
            "evaluation  |  NOUN  |  evaluation\n",
            ",  |  PUNCT  |  ,\n",
            "contributing  |  VERB  |  contribute\n",
            "to  |  ADP  |  to\n",
            "the  |  DET  |  the\n",
            "effectiveness  |  NOUN  |  effectiveness\n",
            "and  |  CCONJ  |  and\n",
            "clarity  |  NOUN  |  clarity\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "of  |  ADP  |  of\n",
            "the  |  DET  |  the\n",
            "code  |  NOUN  |  code\n",
            "implementation  |  NOUN  |  implementation\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "Movies  |  PROPN  |  Movies\n",
            "Recommendation  |  PROPN  |  Recommendation\n",
            "System  |  PROPN  |  System\n",
            "(  |  PUNCT  |  (\n",
            "TMDB  |  PROPN  |  TMDB\n",
            "dataset  |  NOUN  |  dataset\n",
            ")  |  PUNCT  |  )\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Effective  |  X  |  •effective\n",
            "Data  |  PROPN  |  Data\n",
            "Preparation  |  PROPN  |  Preparation\n",
            ":  |  PUNCT  |  :\n",
            "Merging  |  NOUN  |  merging\n",
            "and  |  CCONJ  |  and\n",
            "cleaning  |  VERB  |  clean\n",
            "datasets  |  NOUN  |  dataset\n",
            "while  |  SCONJ  |  while\n",
            "handling  |  VERB  |  handle\n",
            "missing  |  VERB  |  miss\n",
            "values  |  NOUN  |  value\n",
            "and  |  CCONJ  |  and\n",
            "duplicates  |  NOUN  |  duplicate\n",
            ",  |  PUNCT  |  ,\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "ensuring  |  VERB  |  ensure\n",
            "reliable  |  ADJ  |  reliable\n",
            "data  |  NOUN  |  datum\n",
            "for  |  ADP  |  for\n",
            "NLP  |  PROPN  |  NLP\n",
            "tasks  |  NOUN  |  task\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Selective  |  ADJ  |  •selective\n",
            "Feature  |  PROPN  |  Feature\n",
            "Extraction  |  PROPN  |  Extraction\n",
            ":  |  PUNCT  |  :\n",
            "Careful  |  ADJ  |  careful\n",
            "selection  |  NOUN  |  selection\n",
            "of  |  ADP  |  of\n",
            "relevant  |  ADJ  |  relevant\n",
            "columns  |  NOUN  |  column\n",
            "for  |  ADP  |  for\n",
            "movie  |  NOUN  |  movie\n",
            "tag  |  NOUN  |  tag\n",
            "creation  |  NOUN  |  creation\n",
            ",  |  PUNCT  |  ,\n",
            "including  |  VERB  |  include\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "'  |  PROPN  |  '\n",
            "movie_id  |  NOUN  |  movie_id\n",
            "'  |  PROPN  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "title  |  NOUN  |  title\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "overview  |  NOUN  |  overview\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "genres  |  NOUN  |  genre\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "keywords  |  NOUN  |  keyword\n",
            "'  |  VERB  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "cast  |  NOUN  |  cast\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "and  |  CCONJ  |  and\n",
            "'  |  PUNCT  |  '\n",
            "crew  |  NOUN  |  crew\n",
            "'  |  PUNCT  |  '\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Advanced  |  PROPN  |  •Advanced\n",
            "Text  |  PROPN  |  Text\n",
            "Processing  |  NOUN  |  processing\n",
            ":  |  PUNCT  |  :\n",
            "Lowercasing  |  PROPN  |  Lowercasing\n",
            ",  |  PUNCT  |  ,\n",
            "stemming  |  VERB  |  stem\n",
            "with  |  ADP  |  with\n",
            "PorterStemmer  |  PROPN  |  PorterStemmer\n",
            ",  |  PUNCT  |  ,\n",
            "and  |  CCONJ  |  and\n",
            "tokenizing  |  VERB  |  tokenize\n",
            "for  |  ADP  |  for\n",
            "optimal  |  ADJ  |  optimal\n",
            "text  |  NOUN  |  text\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "data  |  NOUN  |  datum\n",
            "preparation  |  NOUN  |  preparation\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Semantic  |  PROPN  |  •Semantic\n",
            "Tag  |  PROPN  |  Tag\n",
            "Generation  |  NOUN  |  generation\n",
            ":  |  PUNCT  |  :\n",
            "Comprehensive  |  ADJ  |  comprehensive\n",
            "movie  |  NOUN  |  movie\n",
            "tags  |  NOUN  |  tag\n",
            "created  |  VERB  |  create\n",
            "by  |  ADP  |  by\n",
            "intelligently  |  ADV  |  intelligently\n",
            "combining  |  VERB  |  combine\n",
            "'  |  PUNCT  |  '\n",
            "overview  |  NOUN  |  overview\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "'  |  PUNCT  |  '\n",
            "genres  |  NOUN  |  genre\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "keywords  |  NOUN  |  keyword\n",
            "'  |  VERB  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "'  |  PUNCT  |  '\n",
            "cast  |  NOUN  |  cast\n",
            "'  |  PUNCT  |  '\n",
            ",  |  PUNCT  |  ,\n",
            "and  |  CCONJ  |  and\n",
            "'  |  PUNCT  |  '\n",
            "crew  |  NOUN  |  crew\n",
            "'  |  PUNCT  |  '\n",
            "data  |  NOUN  |  datum\n",
            "for  |  ADP  |  for\n",
            "precise  |  ADJ  |  precise\n",
            "similarity  |  NOUN  |  similarity\n",
            "calculations  |  NOUN  |  calculation\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Recommendation  |  NOUN  |  •recommendation\n",
            "System  |  NOUN  |  system\n",
            ":  |  PUNCT  |  :\n",
            "Utilizing  |  VERB  |  utilize\n",
            "cosine  |  NOUN  |  cosine\n",
            "similarity  |  NOUN  |  similarity\n",
            "for  |  ADP  |  for\n",
            "accurate  |  ADJ  |  accurate\n",
            "identification  |  NOUN  |  identification\n",
            "of  |  ADP  |  of\n",
            "similar  |  ADJ  |  similar\n",
            "movies  |  NOUN  |  movie\n",
            "based  |  VERB  |  base\n",
            "on  |  ADP  |  on\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "vectorized  |  ADJ  |  vectorized\n",
            "tag  |  NOUN  |  tag\n",
            "representations  |  NOUN  |  representation\n",
            ",  |  PUNCT  |  ,\n",
            "enabling  |  VERB  |  enable\n",
            "personalized  |  ADJ  |  personalized\n",
            "recommendations  |  NOUN  |  recommendation\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "•Deployed  |  VERB  |  •deploye\n",
            "using  |  VERB  |  use\n",
            "streamlit  |  NOUN  |  streamlit\n",
            ".  |  PUNCT  |  .\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n",
            "2  |  NUM  |  2\n",
            "/  |  SYM  |  /\n",
            "2  |  NUM  |  2\n",
            "\n",
            "  |  SPACE  |  \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Word  Entity Type\n",
              "0                                Data Scientist          ORG\n",
              "1   Dwarkadas J. Sanghvi College of Engineering          ORG\n",
              "2                     Btech in Computer Science  WORK_OF_ART\n",
              "3                                          9.38     CARDINAL\n",
              "4                                        1)2021      ORDINAL\n",
              "..                                          ...          ...\n",
              "71                                          NLP          ORG\n",
              "72                           Feature Extraction          ORG\n",
              "73                                PorterStemmer          ORG\n",
              "74                               Tag Generation  WORK_OF_ART\n",
              "75                                            2     CARDINAL\n",
              "\n",
              "[76 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-962b1af5-d4c7-4ab5-b0ec-35f1a16d6075\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Entity Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dwarkadas J. Sanghvi College of Engineering</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Btech in Computer Science</td>\n",
              "      <td>WORK_OF_ART</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.38</td>\n",
              "      <td>CARDINAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1)2021</td>\n",
              "      <td>ORDINAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>NLP</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Feature Extraction</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>PorterStemmer</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Tag Generation</td>\n",
              "      <td>WORK_OF_ART</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2</td>\n",
              "      <td>CARDINAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-962b1af5-d4c7-4ab5-b0ec-35f1a16d6075')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-972864e6-fa66-4ba3-90f0-f9a6235fc50d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-972864e6-fa66-4ba3-90f0-f9a6235fc50d')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-972864e6-fa66-4ba3-90f0-f9a6235fc50d button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-962b1af5-d4c7-4ab5-b0ec-35f1a16d6075 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-962b1af5-d4c7-4ab5-b0ec-35f1a16d6075');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RgO4xn2yKZf",
        "outputId": "5ea759ed-3c32-4a2b-fdad-534ec632b44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycsZLBoFtx1l",
        "outputId": "6cdf63a6-e764-4908-fed9-27767a097591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-11 05:23:30.297163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-trf==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.5.0/en_core_web_trf-3.5.0-py3-none-any.whl (460.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.3/460.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.5.0) (3.5.4)\n",
            "Collecting spacy-transformers<1.3.0,>=1.2.0.dev0 (from en-core-web-trf==3.5.0)\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.3.0)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2.0.1+cu118)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0)\n",
            "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.3.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2023.6.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.3.0)\n",
            "Installing collected packages: spacy-alignments, transformers, spacy-transformers, en-core-web-trf\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0\n",
            "    Uninstalling transformers-4.31.0:\n",
            "      Successfully uninstalled transformers-4.31.0\n",
            "Successfully installed en-core-web-trf-3.5.0 spacy-alignments-0.9.0 spacy-transformers-1.2.5 transformers-4.30.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_data = [\n",
        "    ('Om Manish Uskaikar', 'Person'),\n",
        "    ('Data Scientist', 'Job Title'),\n",
        "    ('omuskaikar@gmail.com', 'Email'),\n",
        "    ('+919326334540', 'Phone Number'),\n",
        "    ('EDUCATION', 'Category Header'),\n",
        "    ('Dwarkadas J. Sanghvi College of Engineering', 'College/University'),\n",
        "    ('Btech in Computer Science and Engineering(Data Science)', 'Education Program'),\n",
        "    ('SGPA', 'Abbreviation'),\n",
        "    ('9.38 (sem 1)', 'GPA Score'),\n",
        "    ('2021 – 2025', 'Date Range'),\n",
        "    ('mumbai, India', 'Location'),\n",
        "    ('SGPA', 'Abbreviation'),\n",
        "    ('9.43 (sem 2)', 'GPA Score'),\n",
        "    ('SGPA', 'Abbreviation'),\n",
        "    ('9.71 (sem 3)', 'GPA Score'),\n",
        "    ('Nirmala Memorial Foundation Jr. College Of Commerce Science', 'College/University'),\n",
        "    ('HSC', 'Abbreviation'),\n",
        "    ('95.12%', 'Percentage'),\n",
        "    ('2019 – 2021', 'Date Range'),\n",
        "    ('mumbai, India', 'Location'),\n",
        "    ('S.T. Lawrence High School', 'School'),\n",
        "    ('SSC', 'Abbreviation'),\n",
        "    ('90.40%', 'Percentage'),\n",
        "    ('2009 – 2019', 'Date Range'),\n",
        "    ('mumbai, India', 'Location'),\n",
        "    ('PROFESSIONAL EXPERIENCE', 'Category Header'),\n",
        "    ('J.P Morgan External Engagement Program', 'Work Experience'),\n",
        "    ('Kaggle Learn', 'Course Name'),\n",
        "    ('(2022)', 'Date'),\n",
        "    ('Kaggle Learn', 'Course Name'),\n",
        "    ('(2022)', 'Date'),\n",
        "    ('Kaggle Learn', 'Course Name'),\n",
        "    ('(2022)', 'Date'),\n",
        "    ('AWS Academy Graduate - AWS Academy Cloud Foundations', 'Certificate'),\n",
        "    ('(2023)', 'Date'),\n",
        "    ('PROFILE', 'Category Header'),\n",
        "    ('PROGRAMMING SKILLS', 'Category Header'),\n",
        "    ('Python programming', 'Skill'),\n",
        "    ('MERN stack', 'Skill'),\n",
        "    ('Machine Learning', 'Skill'),\n",
        "    ('Sklearn', 'Skill'),\n",
        "    ('Numpy', 'Skill'),\n",
        "    ('Pandas', 'Skill'),\n",
        "    ('Matplotlib', 'Skill'),\n",
        "    ('Pickle', 'Skill'),\n",
        "    ('Seaborn', 'Skill'),\n",
        "    ('C programming', 'Skill'),\n",
        "    ('Tableau', 'Skill'),\n",
        "    ('HTML', 'Skill'),\n",
        "    ('CSS', 'Skill'),\n",
        "    ('JavaScript', 'Skill'),\n",
        "    ('My SQL', 'Skill'),\n",
        "    ('PROJECTS', 'Category Header'),\n",
        "    ('Global Analysis of Fatal Diseases', 'Project Name'),\n",
        "    ('Analysis', 'Sub-Project'),\n",
        "    ('Pandas, numpy, matplotlib, seaborn, barchart race', 'Tools/Libraries'),\n",
        "    ('Profit Prediction for Super Store', 'Project Name'),\n",
        "    ('Logistic Regression, Decision Tree, Random Forest, AdaBoost, LightGBM, XGBoost, SVM, KNN', 'Tools/Models'),\n",
        "    ('Linear Regression, Decision Tree Regressor, Random Forest Regressor, AdaBoost Regressor, LightGBM Regressor', 'Tools/Models'),\n",
        "    ('GridSearchCV, RandomizedSearchCV', 'Tools/Libraries'),\n",
        "    ('scikit-learn, XGBoost, LightGBM, pandas, plotly', 'Tools/Libraries'),\n",
        "    ('Movies Recommendation System (TMDB dataset)', 'Project Name'),\n",
        "    (\"'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'\", 'Columns'),\n",
        "    ('PorterStemmer', 'Tool'),\n",
        "    ('streamlit', 'Tool')\n",
        "]"
      ],
      "metadata": {
        "id": "BogGZ5fqr6aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install git+https://github.com/promptslab/Promptify.git"
      ],
      "metadata": {
        "id": "WNsyW15vztU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from promptify import Prompter,OpenAI, Pipeline\n",
        "\n",
        "\n",
        "# Define the API key for the OpenAI model\n",
        "# Create an instance of the OpenAI model, Currently supporting Openai's all model, In future adding more generative models from Hugginface and other platforms\n",
        "model        = OpenAI(\"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\")\n",
        "prompter     = Prompter('ner.jinja')\n",
        "pipe         = Pipeline(prompter , model)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er58aKLW-H_s",
        "outputId": "aa93bb69-6bd5-4947-e147-89c85cfafe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Om Manish Uskaikar\n",
            "Data Scientist\n",
            "omuskaikar@gmail.com +919326334540 omuskaikar\n",
            "EDUCATION\n",
            "Dwarkadas J. Sanghvi College of Engineering\n",
            "Btech in Computer Science and Engineering(Data Science)\n",
            "•SGPA : 9.38 (sem 1)2021 – 2025\n",
            "mumbai, India\n",
            "•SGPA : 9.43 (sem 2)\n",
            "•SGPA : 9.71  (sem 3)\n",
            "Nirmala Memorial Foundation Jr .College Of Commerce Science\n",
            "•HSC : 95.12%2019 – 2021\n",
            "mumbai, India\n",
            "S.T. Lawrence High School\n",
            "•SSC : 90.40%2009 – 2019\n",
            "mumbai, India\n",
            "PROFESSIONAL EXPERIENCE\n",
            "J.P Morgan External Engagement Program\n",
            "A program started by J.P Morgan for top 60 students in engineering colleges to \n",
            "learn and develop professional skills.present\n",
            "CERTIFICATES\n",
            "Python Course (Kaggle Learn)                                                                                                                                 (2022)\n",
            "Intro to Machine Learning(Kaggle Learn)                                                                                                            (2022)\n",
            "Data Visualizations (Kaggle Learn)                                                                                                                        (2022)\n",
            "AWS Academy Graduate - AWS Academy Cloud Foundations                                                                        (2023)\n",
            "PROFILE\n",
            "PROGRAMMING SKILLS\n",
            "Python programming\n",
            "MERN stack\n",
            "Machine Learning\n",
            "(Sklearn , Numpy , Pandas , \n",
            "Matplotlib , Pickle , Seaborn)C programming\n",
            "TableauHTML , CSS , JavaScript\n",
            "My SQL\n",
            "PROJECTS\n",
            "Global Analysis of Fatal Diseases\n",
            "• A data visualization project on analyzing impact of fatal diseases on various countries. Analysis obtained \n",
            "can generate useful insights into spread and prevalence of diseases in different regions and analyzing their \n",
            "historical trends.\n",
            "1 / 2•Currently working to publish a research paper regarding the same.\n",
            "•Performed CRUD operations with SQL on the processed dataset.\n",
            "•Libraries used: Pandas, numpy, matplotlib, seaborn, barchart race\n",
            "Profit Prediction for Super Store\n",
            "•The project required predicting shop profitability and estimation of profit values. This was achieved by \n",
            "applying diverse set of classification models (Logistic Regression, Decision Tree, Random Forest, AdaBoost, \n",
            "LightGBM, XGBoost, SVM, KNN) and regression models (Linear Regression, Decision Tree Regressor, \n",
            "Random Forest Regressor, AdaBoost Regressor, LightGBM Regressor).\n",
            "•Robust data preprocessing techniques were employed, such as label encoding for categorical features and \n",
            "outlier removal based on Z-scores, ensuring data quality and improving model performance and reliability.\n",
            "•Accurate evaluation of models were achieved by calculating metrics like accuracy for classification and root \n",
            "mean squared error (RMSE) for regression, enabling effective comparison and selection of the best-\n",
            "performing model.\n",
            "•Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV, optimized model \n",
            "parameters, enhancing performance and predictive accuracy.\n",
            "•The code showcases the use of pipelines to streamline data preprocessing and modelling, automating the \n",
            "workflow and improving code readability and maintainability.\n",
            "•Utilization of powerful libraries (scikit-learn, XGBoost, LightGBM, pandas, plotly) provided efficient tools \n",
            "for data manipulation, modelling, visualization, and evaluation, contributing to the effectiveness and clarity \n",
            "of the code implementation.\n",
            "Movies Recommendation System (TMDB dataset)\n",
            "•Effective Data Preparation: Merging and cleaning datasets while handling missing values and duplicates, \n",
            "ensuring reliable data for NLP tasks.\n",
            "•Selective Feature Extraction: Careful selection of relevant columns for movie tag creation, including \n",
            "'movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', and 'crew'.\n",
            "•Advanced Text Processing: Lowercasing, stemming with PorterStemmer, and tokenizing for optimal text \n",
            "data preparation.\n",
            "•Semantic Tag Generation: Comprehensive movie tags created by intelligently combining 'overview', \n",
            "'genres', 'keywords', 'cast', and 'crew' data for precise similarity calculations.\n",
            "•Recommendation System: Utilizing cosine similarity for accurate identification of similar movies based on \n",
            "vectorized tag representations, enabling personalized recommendations.\n",
            "•Deployed using streamlit.\n",
            "2 / 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPfvTqjJ_SQs",
        "outputId": "39d69d14-322f-4389-94e1-367d22931145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBYK7Ai5_UJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}